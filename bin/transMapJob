#!/usr/bin/env python3

import transMapProgSetup  # noqa: F401
import sys
import os
import argparse
import pipettor
import logging
from collections import deque
from pycbio.hgdata.coords import Coords
from pycbio.sys import fileOps
from pycbio.hgdata.psl import PslTbl
from pycbio.db import sqliteOps
from transMap import setSortLocale
from transMap import alignIdToSrcId, alignIdDropOneUniq
from transMap.genomeData import AnnotationType, ChainType, GenomesDbTables, GenomeAsmsSqliteTable
from transMap.srcData import SourceDbTables, SrcAlignSqliteTable, SrcMetadataSqliteTable
from transMap.bigTransMap import bigTransMapMakeRec
from pycbio.hgdata.sequenceSqlite import SequenceSqliteTable
from transMap.mappingChainData import MappingChainsDbTables, MappingChainsIndexSqliteTable
from transMap.transMapConf import transMapConfLoad


def parseArgs():
    desc = """Run transmap algorithm on a range of alignment OIDs, do
    all work, including filtering, recalculating match stats, etc.
    """
    parser = argparse.ArgumentParser(description=desc)
    transMapProgSetup.addCmdOptions(parser)
    parser.add_argument("--keep", action="store_true",
                        help="""keep temporary files""")
    parser.add_argument("configPyFile",
                        help="""configuration python file""")
    parser.add_argument("srcHgDb",
                        help="""mysql database for source assembly""")
    parser.add_argument("destHgDb",
                        help="""destination browser database """)
    parser.add_argument("annotationType", choices=(AnnotationType.rna, AnnotationType.est, AnnotationType.refseq, AnnotationType.ensembl), type=AnnotationType,
                        help="""annotation set type""")
    parser.add_argument("chainType", choices=ChainType, type=ChainType,
                        help="""type of chains to use""")
    parser.add_argument("startOid",
                        help="""object id of start row in srcDb.srcAligns""")
    parser.add_argument("endOid",
                        help="""object id of one past last row in srcDb.srcAligns""")
    parser.add_argument("mappedPreBigPsl",
                        help="""output for input to bedToBigBed""")
    opts = parser.parse_args()
    transMapProgSetup.setupFromCmd(opts)
    return opts


def getSrcGenomeAsm(conf):
    """get GenomeAsm record for srcDb"""
    conn = sqliteOps.connect(conf.genomeDb)
    genomeAsmTbl = GenomeAsmsSqliteTable(conn, GenomesDbTables.genomeAsmsTbl)
    recs = list(genomeAsmTbl.queryByHdDb(conf.srcHgDb))
    if len(recs) != 1:
        raise Exception("expected 1 record for {} {} got {} record".format(conf.genomeDb, conf.srcHgDb, len(recs)))
    conn.close()
    return recs[0]


class SrcPslGroups(dict):
    """"Load and split source PSLs into a lists of range overlapping PSLs.
    Object is index by Coords. This prevents reading unnecessary chains.  Doing
    this is the cluster jobs simplifies batch setup."""
    def __init__(self, srcDbConn, startOid, endOid):
        pslq = deque(SrcAlignSqliteTable(srcDbConn, SourceDbTables.srcAlignTbl).getByOid(startOid, endOid))
        self._groupAligns(pslq)

    def _groupAligns(self, pslq):
        while len(pslq) > 0:
            coords, pslGroup = self._buildOneGroup(pslq)
            assert coords not in self
            self[coords] = pslGroup

    @staticmethod
    def _pslRangeOverlaps(tName, tStart, tEnd, psl):
        return (tName == psl.tName) and (tStart < psl.tEnd) and (tEnd > psl.tStart)

    @staticmethod
    def _buildOneGroup(pslq):
        "group psls, removing from queue"

        # seed with first psl, then iterate until we have all overlapping
        # expanding range or we have consumed all
        tName, tStart, tEnd = pslq[0].tName, pslq[0].tStart, pslq[0].tEnd
        pslGroup = [pslq.popleft()]
        processedSome = True
        while (len(pslq) > 0) and processedSome:
            processedSome = False
            i = 0
            while i < len(pslq):
                if SrcPslGroups._pslRangeOverlaps(tName, tStart, tEnd, pslq[i]):
                    psl = SrcPslGroups._dequeue(pslq, i)
                    pslGroup.append(psl)
                    tStart = min(psl.tStart, tStart)
                    tEnd = max(psl.tEnd, tEnd)
                    processedSome = True
                i += 1
        return Coords(tName, tStart, tEnd), pslGroup

    @staticmethod
    def _dequeue(pslq, idx):
        "remove one psl in an efficient way"
        psl = pslq[idx]
        if idx < len(pslq) - 1:
            pslq[idx] = pslq.pop()  # replace with last element
        del pslq[-1]
        return psl


class MappingChains(object):
    def __init__(self, mappingChainsFile, chainType):
        self.conn = sqliteOps.connect("{}.db".format(mappingChainsFile))
        self.chainType = chainType
        self.indexTbl = MappingChainsIndexSqliteTable(self.conn, MappingChainsDbTables.mappingChainsTbl)
        self.fh = open(mappingChainsFile)

    def dumpToTmpFile(self, coords, tmpFile):
        with open(tmpFile, "w") as tmpFh:
            return self._dumpToTmpFile(coords, tmpFh)

    def _dumpToTmpFile(self, coords, tmpFh):
        cnt = 0
        for loc in self.indexTbl.getRangeOverlap(coords.name, coords.start, coords.end):
            self._dumpOneChain(loc, tmpFh)
            cnt += 1
        logging.debug("chains: {} chains for {}".format(cnt, coords))
        return cnt

    def _dumpOneChain(self, loc, tmpFh):
        self.fh.seek(loc.offset)
        buf = self.fh.read(loc.length)
        if len(buf) != loc.length:
            raise Exception("expected {} chain bytes, got {}".format(loc.length, len(buf)))
        tmpFh.write(buf)


class PslMapper(object):
    def __init__(self, conf, srcHgDb, destHgDb, srcMetadataTbl, srcSeqTbl, mappingChains, keep):
        self.conf = conf
        self.srcHgDb = srcHgDb
        self.srcGenomeAsm = getSrcGenomeAsm(conf)
        self.destHgDb = destHgDb
        self.srcMetadataTbl = srcMetadataTbl
        self.srcSeqTbl = srcSeqTbl
        self.mappingChains = mappingChains
        self.keep = keep

    def _writeSrcPsls(self, srcPsls):
        srcPslTmpFile = fileOps.tmpFileGet("transMap.src.", ".psl")
        with open(srcPslTmpFile, "w") as pslFh:
            for psl in srcPsls:
                psl.write(pslFh)
        return srcPslTmpFile

    def _writeSrcSeq(self, srcId, faFh):
        srcSeq = self.srcSeqTbl.getByName(srcId)
        faFh.write(srcSeq.toFasta())

    def _getUniqPslSrcId(self, srcPsls):
        return set([alignIdToSrcId(p.qName) for p in srcPsls])

    def _writeSrcSeqs(self, srcPsls):
        srcSeqsTmpFile = fileOps.tmpFileGet("transMap.src.", ".fa")
        with open(srcSeqsTmpFile, "w") as faFh:
            for srcId in self._getUniqPslSrcId(srcPsls):
                self._writeSrcSeq(srcId, faFh)
        return srcSeqsTmpFile

    def _writeMappingChains(self, srcPslsCoords):
        "return None is no chains are written"
        chainsTmpFile = fileOps.tmpFileGet("transMap.mapping.", ".chain")
        cnt = self.mappingChains.dumpToTmpFile(srcPslsCoords, chainsTmpFile)
        if cnt > 0:
            return chainsTmpFile
        else:
            os.unlink(chainsTmpFile)
            return None

    def _mapPslsToDestWithChains(self, srcPslsCoords, srcPsls, chainsTmpFile):
        srcPslTmpFile = self._writeSrcPsls(srcPsls)
        srcSeqsTmpFile = self._writeSrcSeqs(srcPsls)
        mappedPslTmpFile = fileOps.tmpFileGet("transMap.mapped.", ".psl")
        pslMapCmd = ["pslMap", "-chainMapFile",
                     srcPslTmpFile, chainsTmpFile, "stdout"]
        pslQueryUniqCmd = ["pslQueryUniq"]
        pslCDnaFilterCmd = ["pslCDnaFilter", "-verbose=0",
                            "-minQSize={}".format(self.conf.mappedMinQSize),
                            "-maxAligns={}".format(self.conf.mappedMaxAligns),
                            "-minCover={}".format(self.conf.mappedMinCover),
                            "-globalNearBest={}".format(self.conf.mappedGlobalNearBest),
                            "stdin", "stdout"]
        tsortCmd = ["sort", "-k", "14,14", "-k", "16,16n"]
        pslRecalcMatchCmd = ["pslRecalcMatch", "-ignoreQUniq", "stdin",
                             self.conf.getDestTwoBit(), srcSeqsTmpFile, "stdout"]
        cmds = [pslMapCmd, pslCDnaFilterCmd, tsortCmd, pslRecalcMatchCmd, pslQueryUniqCmd]
        p = pipettor.Pipeline(cmds, stdout=mappedPslTmpFile)
        p.wait()
        mappedPsls = PslTbl(mappedPslTmpFile)
        tmpFiles = [srcPslTmpFile, srcSeqsTmpFile, chainsTmpFile, mappedPslTmpFile]
        if self.keep:
            sys.stderr.write("keep files: {}\n".format(" ".join(tmpFiles)))
        else:
            fileOps.rmFiles(*tmpFiles)
        return mappedPsls

    def _mapPslsToDest(self, srcPslsCoords, srcPsls):
        chainsTmpFile = self._writeMappingChains(srcPslsCoords)
        if chainsTmpFile is not None:
            return self._mapPslsToDestWithChains(srcPslsCoords, srcPsls, chainsTmpFile)
        else:
            return []  # no chains

    @staticmethod
    def _findSrcPsl(srcPsls, mappedPsl):
        srcQName = alignIdDropOneUniq(mappedPsl.qName)
        for srcPsl in srcPsls:
            if srcPsl.qName == srcQName:
                return srcPsl
        raise Exception("BUG: __findSrcPsl: srcPsl not found for {}".format(srcQName))

    def _writeBigPslRec(self, srcPsls, mappedPsl, mappedPreBigPslFh):
        srcId = alignIdToSrcId(mappedPsl.qName)
        srcSeq = self.srcSeqTbl.getByName(srcId)
        srcPsl = self._findSrcPsl(srcPsls, mappedPsl)
        metaData = self.srcMetadataTbl.getBySrcId(srcId) if self.srcMetadataTbl is not None else None
        fileOps.prRow(mappedPreBigPslFh,
                      bigTransMapMakeRec(self.srcHgDb, srcPsl, mappedPsl, srcSeq,
                                         self.mappingChains.chainType, metaData,
                                         self.srcGenomeAsm.commonName,
                                         self.srcGenomeAsm.scientificName,
                                         self.srcGenomeAsm.orgAbbrev))

    def mapPslGroup(self, srcPslsCoords, srcPsls, mappedPreBigPslFh):
        mappedPsls = self._mapPslsToDest(srcPslsCoords, srcPsls)
        for mappedPsl in mappedPsls:
            self._writeBigPslRec(srcPsls, mappedPsl, mappedPreBigPslFh)


def transMapJob(opts):
    conf = transMapConfLoad(opts.configPyFile, srcHgDb=opts.srcHgDb,
                            destHgDb=opts.destHgDb,
                            annotationType=opts.annotationType,
                            chainType=opts.chainType)
    srcDbConn = sqliteOps.connect(conf.srcDb)
    if sqliteOps.haveTable(srcDbConn, SourceDbTables.srcMetadataTbl):
        srcMetadataTbl = SrcMetadataSqliteTable(srcDbConn, SourceDbTables.srcMetadataTbl)
    else:
        srcMetadataTbl = None  # ESTs
    srcSeqTbl = SequenceSqliteTable(srcDbConn, SourceDbTables.srcSeqTbl)
    mappingChains = MappingChains(conf.mappingChains, conf.chainType)
    srcPslGroups = SrcPslGroups(srcDbConn, opts.startOid, opts.endOid)

    pslMapper = PslMapper(conf, opts.srcHgDb, opts.destHgDb, srcMetadataTbl, srcSeqTbl, mappingChains, opts.keep)
    srcPslGroups = SrcPslGroups(srcDbConn, opts.startOid, opts.endOid)

    # fields 3,4 are for test stability
    sortBedCmd = ["sort", "-k1,1", "-k2,2n", "-k3,3n", "-k4,4"]

    fileOps.ensureFileDir(opts.mappedPreBigPsl)
    with fileOps.AtomicFileCreate(opts.mappedPreBigPsl) as tmpMappedPreBigPsl:
        with pipettor.Popen([sortBedCmd], "w", stdout=tmpMappedPreBigPsl) as mappedPreBigPslFh:
            for srcPslsCoords in srcPslGroups.keys():
                pslMapper.mapPslGroup(srcPslsCoords, srcPslGroups[srcPslsCoords], mappedPreBigPslFh)


setSortLocale()
transMapJob(parseArgs())
